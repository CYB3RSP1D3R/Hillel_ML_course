{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Імпорт необіхдних бібліотек"
      ],
      "metadata": {
        "id": "4N0PRqi6yTv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4VRFgQUBxtao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import itertools\n",
        "from timeit import timeit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зафіксуємо seed генератору випадкових чисел"
      ],
      "metadata": {
        "id": "r86tgiVxyYSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(17)"
      ],
      "metadata": {
        "id": "7yml_0gryfVS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Викачуємо датасет MNIST, попередньо розділивши його на сукупність для тренування та тесту"
      ],
      "metadata": {
        "id": "0xcvBVRLynTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(data_train, data_test), data_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ],
      "metadata": {
        "id": "b-ZghPULyn9H"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Виводимо інформацію про датасет"
      ],
      "metadata": {
        "id": "X0sigoR8zENc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-k4AwRgy6Po",
        "outputId": "e099df52-8b77-407f-db69-2f6a0319b6bc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    full_name='mnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    The MNIST database of handwritten digits.\n",
            "    \"\"\",\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    data_dir='/root/tensorflow_datasets/mnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=11.06 MiB,\n",
            "    dataset_size=21.00 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будуємо функцію побудови моделі"
      ],
      "metadata": {
        "id": "NM7MqGwhzIB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_model(\n",
        "    cnn_num_kernels,\n",
        "    cnn_kernel_size,\n",
        "    cnn_maxpool,\n",
        "    nn_size,\n",
        "    dropout,\n",
        "    output_num,\n",
        "    optimizer,\n",
        "    loss,\n",
        "    metrics,\n",
        "):\n",
        "    \"\"\"Function for building a sequencial model for MNIST dataset.\n",
        "    Inputs:\n",
        "        cnn_num_kernels:    NDArray[k] -- number of kernels for each layer\n",
        "        cnn_kernel_size:    int        -- size of one side of kernel for each layer\n",
        "        cnn_maxpool:        int        -- size of one side of maxpool reduction for each layer\n",
        "        nn_size:            NDArray[n] -- size of each dense layer\n",
        "        dropout:            float,     -- fraction of connections that will be dropped out\n",
        "        output_num:         int,       -- number of output classes\n",
        "        optimizer:          str,       -- optimizer to be used\n",
        "        loss:               str,       -- loss to be used\n",
        "        metrics:            list[str]  -- list of metrics to be used\n",
        "    Output:\n",
        "        model:              Sequental  -- output model\n",
        "\n",
        "    \"\"\"\n",
        "    # Sequencial model is enough for MNIST\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Convolutional part\n",
        "    for kernel_num in cnn_num_kernels:\n",
        "        model.add(\n",
        "            tf.keras.layers.Conv2D(\n",
        "                kernel_num,\n",
        "                (cnn_kernel_size, cnn_kernel_size),\n",
        "                activation='relu'\n",
        "            )\n",
        "        )\n",
        "        model.add(tf.keras.layers.MaxPooling2D((cnn_maxpool, cnn_maxpool)))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    # Dense part\n",
        "    for size in nn_size:\n",
        "        model.add(tf.keras.layers.Dense(size, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dropout(dropout))\n",
        "    model.add(tf.keras.layers.Dense(output_num, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "43BALRvo5P-n"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаємо batch size, та кількість epoch."
      ],
      "metadata": {
        "id": "q-DqCbov-6rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "hfuloLklK5by"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будуємо навчальний конвеєр:\n",
        "\n",
        "    1. Переводимо дані з `uint8` [0, 255] у `float32` ([0.0, 1.0])\n",
        "    2. Зберігаємо дані в кеш перед перетасуванням\n",
        "    3. Перетасовуємо дані для рівномірного розподілення, для найкращого результату перетасуємо одночасно всі зображення.\n",
        "    4. Розділяємо перетасований датасет на пакети (batches) фіксованого розміру для пакетного стохастичного градієнтного спуску\n",
        "    5. Попередньо завантажуємо дані для кращої продуктивності\n",
        "\n",
        "Функції запозичено з [прикладу для Keras](https://www.tensorflow.org/datasets/keras_example)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9s0bDzZp0Gih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  assert image.dtype == tf.uint8\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "data_train = data_train.map(\n",
        "    normalize_img,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "data_train = data_train.cache()\n",
        "data_train = data_train.shuffle(data_info.splits['train'].num_examples)\n",
        "data_train = data_train.batch(batch_size)\n",
        "data_train = data_train.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "cGyGYCOCzHjj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будуємо тестувальний конвеєр.\n",
        "\n",
        "Для нього вже не потрібно перетасовувати дані."
      ],
      "metadata": {
        "id": "aP-dpUmJ4cBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = data_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "data_test = data_test.batch(batch_size)\n",
        "data_test = data_test.cache()\n",
        "data_test = data_test.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "myb4KB9Nziyi"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаємо параметри, що потрібно змінювати при підборі"
      ],
      "metadata": {
        "id": "JzjCVHXPK8d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_num_kernels = [\n",
        "    [8],\n",
        "    [16],\n",
        "    [32],\n",
        "    [8, 16],\n",
        "    [16, 32],\n",
        "    [32, 64],\n",
        "    [8, 16, 16],\n",
        "    [16, 32, 32],\n",
        "    [32, 64, 64]\n",
        "]\n",
        "cnn_kernel_size = [3, 5, 7]\n",
        "cnn_maxpool = [2, 4]\n",
        "nn_size = [\n",
        "    [64],\n",
        "    [128],\n",
        "    [256],\n",
        "    [64, 128],\n",
        "    [128, 256],\n",
        "    [256, 128],\n",
        "    [128, 64]\n",
        "]"
      ],
      "metadata": {
        "id": "3z9yf3QaMe0m"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаємо фіксовані параметри"
      ],
      "metadata": {
        "id": "WXd1IStBNncs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.5\n",
        "output_num = 10\n",
        "optimizer = 'adam'\n",
        "loss = 'sparse_categorical_crossentropy'\n",
        "metrics = ['sparse_categorical_accuracy']"
      ],
      "metadata": {
        "id": "dW3wP_Km-37y"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Спершу шукаємо параметри нейроної мережі без згорткових шарів."
      ],
      "metadata": {
        "id": "KzltImPeeYba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_opt = None\n",
        "model_opt = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for size in nn_size:\n",
        "    print('Parameters:', size)\n",
        "\n",
        "    model = get_tf_model(\n",
        "        [],\n",
        "        None,\n",
        "        None,\n",
        "        size,\n",
        "        dropout=dropout,\n",
        "        output_num=output_num,\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        data_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    accuracy = model.evaluate(data_test, verbose=2)[1]\n",
        "    if accuracy > best_accuracy:\n",
        "        model_opt = tf.keras.models.clone_model(model)\n",
        "        param_opt = size\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "    print()\n",
        "\n",
        "print('Optimal parameters accuracy:', param_opt)\n",
        "print('Best accuracy:', best_accuracy)\n",
        "print(model_opt.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncDXjF8peh3T",
        "outputId": "217683b1-7934-4173-c4d4-310966a704cf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: [64]\n",
            "79/79 - 0s - loss: 0.1349 - sparse_categorical_accuracy: 0.9609 - 253ms/epoch - 3ms/step\n",
            "\n",
            "Parameters: [128]\n",
            "79/79 - 0s - loss: 0.0864 - sparse_categorical_accuracy: 0.9744 - 254ms/epoch - 3ms/step\n",
            "\n",
            "Parameters: [256]\n",
            "79/79 - 1s - loss: 0.0675 - sparse_categorical_accuracy: 0.9785 - 546ms/epoch - 7ms/step\n",
            "\n",
            "Parameters: [64, 128]\n",
            "79/79 - 0s - loss: 0.1255 - sparse_categorical_accuracy: 0.9628 - 244ms/epoch - 3ms/step\n",
            "\n",
            "Parameters: [128, 256]\n",
            "79/79 - 0s - loss: 0.0894 - sparse_categorical_accuracy: 0.9734 - 279ms/epoch - 4ms/step\n",
            "\n",
            "Parameters: [256, 128]\n",
            "79/79 - 0s - loss: 0.0742 - sparse_categorical_accuracy: 0.9779 - 317ms/epoch - 4ms/step\n",
            "\n",
            "Parameters: [128, 64]\n",
            "79/79 - 0s - loss: 0.1013 - sparse_categorical_accuracy: 0.9706 - 279ms/epoch - 4ms/step\n",
            "\n",
            "Optimal parameters accuracy: [256]\n",
            "Best accuracy: 0.9785000085830688\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_30 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 256)               200960    \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 203530 (795.04 KB)\n",
            "Trainable params: 203530 (795.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шукаємо серед множини параметрів той, що дасть найкращу тестову точність і зберігаємо його.\n",
        "\n",
        "Паралельно шукаємо набір параметрів, що дасть нам саме оптимальне відношення точності на одиницю продуктивності."
      ],
      "metadata": {
        "id": "F2KO6A4KNv4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_opt = None\n",
        "model_opt = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Let us track accuracy / perfrormance ratio\n",
        "param_opt_perf = None\n",
        "model_opt_perf = None\n",
        "exec_time = None\n",
        "best_acc_perf_ratio = 0.0\n",
        "\n",
        "# Perform cartesian product of four lists\n",
        "for param in itertools.product(cnn_num_kernels, cnn_kernel_size, cnn_maxpool, nn_size):\n",
        "\n",
        "    print('Parameters:', param)\n",
        "\n",
        "    model = get_tf_model(\n",
        "        cnn_num_kernels=param[0],\n",
        "        cnn_kernel_size=param[1],\n",
        "        cnn_maxpool=param[2],\n",
        "        nn_size=param[3],\n",
        "        dropout=dropout,\n",
        "        output_num=output_num,\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        data_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    accuracy = model.evaluate(data_test, verbose=2)[1]\n",
        "\n",
        "    # In practice, 30 is enough for Law of Large Numbers\n",
        "    time_test = lambda : model.predict(np.zeros((1, 28, 28, 1)), verbose=0)\n",
        "    exec_time = timeit(time_test, number=30) / 30\n",
        "\n",
        "    acc_perf_ratio = accuracy / exec_time\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        model_opt = tf.keras.models.clone_model(model)\n",
        "        param_opt = param\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "    if acc_perf_ratio > best_acc_perf_ratio:\n",
        "        model_opt_perf = tf.keras.models.clone_model(model)\n",
        "        param_opt_perf = param\n",
        "        best_acc_perf_ratio = acc_perf_ratio\n",
        "\n",
        "    print()\n",
        "\n",
        "print('Optimal parameters accuracy:', param_opt)\n",
        "print('Best accuracy:', best_accuracy)\n",
        "print(model_opt.summary())\n",
        "model_opt.save_weights('./model_opt')\n",
        "\n",
        "print('Optimal parameters accuracy / performance ratio:', param_opt)\n",
        "print('Best accuracy per performance:', best_accuracy)\n",
        "print(model_opt.summary())\n",
        "model_opt.save_weights('./model_opt_perf')"
      ],
      "metadata": {
        "id": "TYfX2tE-_MED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "6cc1701d-185f-48b0-daf9-753959a9f2cb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: ([8], 3, 2, [64])\n",
            "79/79 - 1s - loss: 0.0555 - sparse_categorical_accuracy: 0.9810 - 999ms/epoch - 13ms/step\n",
            "Parameters: ([8], 3, 2, [128])\n",
            "79/79 - 1s - loss: 0.0458 - sparse_categorical_accuracy: 0.9849 - 864ms/epoch - 11ms/step\n",
            "Parameters: ([8], 3, 2, [256])\n",
            "79/79 - 1s - loss: 0.0399 - sparse_categorical_accuracy: 0.9857 - 930ms/epoch - 12ms/step\n",
            "Parameters: ([8], 3, 2, [64, 64])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-6aed8f791fba>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}